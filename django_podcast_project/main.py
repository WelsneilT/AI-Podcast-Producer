# main.py - AI Podcast & Storyboard Producer (Local Version 6.0 - Content-Rich)

# ====================================================
# PHáº¦N 1: IMPORT, Cáº¤U HÃŒNH
# ====================================================
print("âš™ï¸ Khá»Ÿi táº¡o há»‡ thá»‘ng sáº£n xuáº¥t video AI...")

import os, re, json, time, requests, shutil, torch, soundfile as sf, ffmpeg
from groq import Groq
from bs4 import BeautifulSoup
from pydub import AudioSegment
from unidecode import unidecode
from diffusers import DiffusionPipeline
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
from dotenv import load_dotenv

load_dotenv()

try:
    groq_api_key = os.getenv("GROQ_API_KEY")
    if not groq_api_key: raise ValueError("GROQ_API_KEY khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong file .env")
    groq_client = Groq(api_key=groq_api_key)
    print("âœ… ÄÃ£ cáº¥u hÃ¬nh Groq API Key.")
except Exception as e:
    print(f"ğŸ›‘ Lá»–I cáº¥u hÃ¬nh API: {e}"); exit()

device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cpu": print("âš ï¸ Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y GPU, chÆ°Æ¡ng trÃ¬nh sáº½ cháº¡y ráº¥t cháº­m.")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ASSETS_DIR = os.path.join(BASE_DIR, "assets")
MODELS_DIR = os.path.join(BASE_DIR, "models")
OUTPUT_DIR = os.path.join(BASE_DIR, "output")
os.makedirs(ASSETS_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"âœ… Thiáº¿t bá»‹: {device.upper()}. ThÆ° má»¥c output: {OUTPUT_DIR}")

tts_model_pack = None
image_gen_pipe = None

# ====================================================
# PHáº¦N 2: THÆ¯ VIá»†N HÃ€M CHá»¨C NÄ‚NG
# ====================================================
print("ğŸ› ï¸ Äang Ä‘á»‹nh nghÄ©a thÆ° viá»‡n hÃ m sáº£n xuáº¥t...")

def load_ai_models():
    global tts_model_pack, image_gen_pipe
    print("\n--- Táº£i cÃ¡c mÃ´ hÃ¬nh AI (chá»‰ láº§n Ä‘áº§u, sáº½ máº¥t nhiá»u thá»i gian) ---")
    if tts_model_pack is None:
        print("ğŸ™ï¸ [TTS] Äang táº£i mÃ´ hÃ¬nh giá»ng nÃ³i...")
        try:
            processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
            model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts").to(device)
            vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to(device)
            embeddings_file_path = os.path.join(MODELS_DIR, "speaker_embeddings.pt")
            if not os.path.exists(embeddings_file_path): raise FileNotFoundError(f"Lá»–I: KhÃ´ng tÃ¬m tháº¥y file '{embeddings_file_path}'.")
            speaker_embeddings = torch.load(embeddings_file_path, map_location=device)
            voices = {"Alex": speaker_embeddings[7306].unsqueeze(0), "Ben": speaker_embeddings[1234].unsqueeze(0)}
            tts_model_pack = {"processor": processor, "model": model, "vocoder": vocoder, "voices": voices}
            print("âœ… [TTS] MÃ´ hÃ¬nh giá»ng nÃ³i Ä‘Ã£ sáºµn sÃ ng.")
        except Exception as e: print(f"ğŸ›‘ Lá»—i khi táº£i mÃ´ hÃ¬nh TTS: {e}"); raise e
    if image_gen_pipe is None:
        print("ğŸ¨ [Image Gen] Äang táº£i mÃ´ hÃ¬nh Dreamshaper 8...")
        try:
            model_id = "Lykon/dreamshaper-8"
            pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True)
            pipe.to(device)
            image_gen_pipe = pipe
            print("âœ… [Image Gen] MÃ´ hÃ¬nh táº¡o áº£nh Ä‘Ã£ sáºµn sÃ ng.")
        except Exception as e: print(f"ğŸ›‘ Lá»—i khi táº£i mÃ´ hÃ¬nh Image Gen: {e}"); raise e
    print("--- Táº¥t cáº£ mÃ´ hÃ¬nh AI Ä‘Ã£ Ä‘Æ°á»£c táº£i ---")

def get_content_from_url(url):
    print(f"  -> ğŸ” [Scraper] Láº¥y ná»™i dung tá»«: {url}")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']): tag.decompose()
        text = " ".join(t for t in soup.get_text(separator=' ').split() if t)
        print("  -> âœ… [Scraper] Láº¥y ná»™i dung thÃ nh cÃ´ng.")
        return text
    except Exception as e: print(f"  -> ğŸ›‘ [Scraper] Lá»—i: {e}"); return None

def generate_podcast_script_from_content(content):
    """NÃ‚NG Cáº¤P: Táº¡o ra má»™t ká»‹ch báº£n há»™i thoáº¡i thá»±c sá»±."""
    print("  -> âœï¸ [LLM-Scriptwriter] Táº¡o ká»‹ch báº£n há»™i thoáº¡i chi tiáº¿t...")
    script_prompt = f"You are a master podcast scriptwriter. Based on the following text, create an engaging and natural-sounding conversational script in ENGLISH for two hosts, Alex (the curious host) and Ben (the expert). The script MUST have at least 8 to 10 back-and-forth conversational turns to make it interesting. Each line MUST strictly start with 'Alex:' or 'Ben:'. Do not add any titles, episode names, or markdown formatting. Just the raw dialogue.\n\nText to base the script on:\n---\n{content[:5000]}\n---"
    try:
        completion = groq_client.chat.completions.create(model="llama3-8b-8192", messages=[{"role": "user", "content": script_prompt}])
        raw_script_text = completion.choices[0].message.content
        cleaned_lines = [f"{m.group(1)}: {m.group(2)}" for line in raw_script_text.split('\n') if (m := re.match(r"^\*?\*?(Alex|Ben)\*?\*?:\s*(.*)", line.strip()))]
        if not cleaned_lines: print("  -> ğŸ›‘ [LLM-Scriptwriter] KhÃ´ng thá»ƒ trÃ­ch xuáº¥t Ä‘Æ°á»£c dÃ²ng thoáº¡i há»£p lá»‡."); return None
        final_script = "\n".join(cleaned_lines)
        print("  -> âœ… [LLM-Scriptwriter] Ká»‹ch báº£n Ä‘Ã£ Ä‘Æ°á»£c táº¡o vÃ  lÃ m sáº¡ch.")
        return final_script
    except Exception as e: print(f"  -> ğŸ›‘ [LLM-Scriptwriter] Lá»—i khi táº¡o ká»‹ch báº£n: {e}"); return None

def create_storyboard_from_script(script_text, overall_style):
    """NÃ‚NG Cáº¤P V3: Ã‰p AI pháº£i xá»­ lÃ½ tá»«ng dÃ²ng thoáº¡i, khÃ´ng Ä‘Æ°á»£c gá»™p."""
    print(f"  -> ğŸ¬ [Director AI] PhÃ¢n tÃ­ch ká»‹ch báº£n Tá»ªNG DÃ’NG má»™t...")
    
    # --- PROMPT "Äáº O DIá»„N" SIÃŠU NGHIÃŠM KHáº®C ---
    system_prompt = f"""
    You are a meticulous AI Film Director. Your task is to process a podcast script LINE BY LINE and create a visual scene for EACH line of dialogue.

    **CRITICAL RULES - YOU MUST FOLLOW THESE EXACTLY:**

    1.  **PROCESS EACH LINE INDIVIDUALLY:** You will be given a script. For **EVERY SINGLE LINE** of dialogue (e.g., "Alex: ...", "Ben: ..."), you must create one corresponding scene object in the output array. DO NOT skip any lines. DO NOT group lines together. One line of dialogue equals one scene.
    2.  **VISUAL STORYTELLING:** When the dialogue **describes** a place, event, or concept, the image prompt **MUST** visualize that concept. DO NOT show the host's face.
    3.  **REACTION SHOTS:** Only generate an image of a host for short, emotional reactions or direct questions (e.g., "Wow, that's amazing!").
    4.  **STYLE CONSISTENCY:** The entire storyboard MUST be in the style of: **'{overall_style}'**. This style MUST be in every image prompt.
    5.  **JSON OUTPUT:** You MUST return a single, valid JSON object with one key: "storyboard". The value MUST be an array of objects. The number of objects in the array must be equal to the number of dialogue lines in the input script.
    6.  **MANDATORY FIELDS:** EACH object in the array MUST contain these THREE keys: "host", "dialogue", and "image_prompt". All keys must have non-empty string values.
    """
    
    try:
        # Láº¥y cÃ¡c dÃ²ng thoáº¡i Ä‘á»ƒ Ä‘Æ°a vÃ o prompt, giÃºp LLM táº­p trung
        dialogue_lines = "\n".join([line for line in script_text.split('\n') if re.match(r"^(Alex|Ben):", line.strip())])
        
        completion = groq_client.chat.completions.create(
            model="llama3-70b-8192",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": f"Here is the script. Process it line by line:\n\n{dialogue_lines}"}],
            response_format={"type": "json_object"}
        )
        
        raw_json = json.loads(completion.choices[0].message.content)
        storyboard_scenes = raw_json.get("storyboard", [])
        
        # Validation váº«n giá»¯ nguyÃªn Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng
        validated_scenes = [s for s in storyboard_scenes if isinstance(s, dict) and all(k in s and s[k] for k in ['host', 'dialogue', 'image_prompt'])]
        
        if not validated_scenes:
            print("  -> ğŸ›‘ [Director AI] KhÃ´ng cÃ³ cáº£nh nÃ o há»£p lá»‡ Ä‘Æ°á»£c táº¡o ra.")
            return None
            
        print(f"  -> âœ… [Director AI] Táº¡o vÃ  xÃ¡c thá»±c {len(validated_scenes)} cáº£nh thÃ nh cÃ´ng!")
        return validated_scenes
    except Exception as e:
        print(f"  -> ğŸ›‘ [Director AI] Lá»—i nghiÃªm trá»ng khi táº¡o storyboard: {e}")
        return None

def generate_image_locally(prompt, filename):
    print(f"    -> ğŸ¨ [Image Gen] Äang váº½ áº£nh: '{prompt[:60]}...'")
    try:
        negative_prompt = "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, bad anatomy, watermark, signature, low quality, text, letters"
        image = image_gen_pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, height=768, width=768).images[0]
        image.save(filename)
        print(f"    -> âœ… [Image Gen] ÄÃ£ lÆ°u áº£nh: {os.path.basename(filename)}"); return filename
    except Exception as e: print(f"    -> ğŸ›‘ [Image Gen] Lá»—i khi táº¡o áº£nh: {e}"); return None

def text_to_speech(text, host, output_path):
    print(f"    -> ğŸ—£ï¸ [TTS] Táº¡o audio cho '{host}': '{text[:50]}...'")
    processed_text = unidecode(text)
    inputs = tts_model_pack["processor"](text=processed_text, return_tensors="pt").to(device)
    speech = tts_model_pack["model"].generate_speech(inputs["input_ids"], tts_model_pack["voices"][host], vocoder=tts_model_pack["vocoder"])
    sf.write(output_path, speech.cpu().numpy(), 16000)
    print(f"    -> âœ… [TTS] ÄÃ£ lÆ°u audio: {os.path.basename(output_path)}"); return output_path

def assemble_video_with_ffmpeg(audio_clips_info, image_paths, assets_dir, output_path):
    print("\nğŸ¬ Báº¯t Ä‘áº§u cÃ´ng Ä‘oáº¡n dá»±ng video vá»›i FFmpeg...")
    video_inputs, audio_inputs = [], []
    TARGET_SIZE = '768x768'
    for i, img_path in enumerate(image_paths):
        if i < len(audio_clips_info) and os.path.exists(img_path):
            duration = audio_clips_info[i]['duration']
            print(f"  -> Chuáº©n bá»‹ cáº£nh {i+1}, thá»i lÆ°á»£ng {duration:.2f} giÃ¢y.")
            video_stream = (ffmpeg.input(img_path, loop=1, t=duration, framerate=24).filter('zoompan', z='min(zoom+0.001,1.15)', d=1, x='iw/2-(iw/zoom/2)', y='ih/2-(ih/zoom/2)', fps=24, s=TARGET_SIZE))
            video_inputs.append(video_stream)
            audio_inputs.append(ffmpeg.input(audio_clips_info[i]['path']))
    if not video_inputs: raise ValueError("KhÃ´ng cÃ³ clip hÃ¬nh áº£nh nÃ o há»£p lá»‡ Ä‘á»ƒ dá»±ng.")
    
    concatenated_video = ffmpeg.concat(*video_inputs, v=1, a=0).node
    concatenated_audio = ffmpeg.concat(*audio_inputs, v=0, a=1).node
    final_audio = concatenated_audio[0]
    try:
        bgm_path = os.path.join(assets_dir, "background.mp3")
        if os.path.exists(bgm_path):
            print("  -> ThÃªm nháº¡c ná»n...")
            bgm_stream = ffmpeg.input(bgm_path, stream_loop=-1).audio
            final_audio = ffmpeg.filter([concatenated_audio[0], bgm_stream], 'amix', inputs=2, duration='first', dropout_transition=1, weights="1 0.15")
    except Exception as e: print(f"  - Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ thÃªm nháº¡c ná»n. Lá»—i: {e}")

    print(f"  -> Äang xuáº¥t video ra file: {output_path}")
    (ffmpeg.output(concatenated_video[0], final_audio, output_path, vcodec='libx264', acodec='aac', shortest=None, pix_fmt='yuv420p').overwrite_output().run(quiet=True))
    print(f"âœ… ÄÃ£ dá»±ng xong video!"); return output_path

# ====================================================
# PHáº¦N 3: DÃ‚Y CHUYá»€N Sáº¢N XUáº¤T CHÃNH
# ====================================================
def run_production_pipeline():
    print("\n" + "="*60 + "\nğŸš€ Báº®T Äáº¦U DÃ‚Y CHUYá»€N Sáº¢N XUáº¤T AUDIO-VISUAL...\n" + "="*60)
    
    URL_INPUT = input(">> Nháº­p URL bÃ i viáº¿t Ä‘á»ƒ táº¡o video: ")
    print(">> Chá»n phong cÃ¡ch nghá»‡ thuáº­t:")
    style_map = {'1': 'cinematic, dramatic, photorealistic', '2': 'Ghibli studio anime style, beautiful scenery', '3': 'impressionistic oil painting, vibrant colors'}
    for key, value in style_map.items(): print(f"   {key}: {value}")
    STYLE_CHOICE = input(">> Lá»±a chá»n cá»§a báº¡n (1, 2, 3): ")
    OVERALL_STYLE = style_map.get(STYLE_CHOICE, 'cinematic, dramatic, photorealistic')
    EPISODE_NAME = input(">> Nháº­p tÃªn cho sáº£n pháº©m (khÃ´ng dáº¥u, khÃ´ng cÃ¡ch): ") or "MyFirstLocalVideo"
    
    print("\n" + "="*60); print(f"ğŸš€ Báº¯t Ä‘áº§u táº¡o sáº£n pháº©m '{EPISODE_NAME}'..."); print("="*60 + "\n")

    episode_output_dir = os.path.join(OUTPUT_DIR, EPISODE_NAME)
    audio_temp_dir = os.path.join(episode_output_dir, 'audio')
    image_temp_dir = os.path.join(episode_output_dir, 'images')
    if os.path.exists(episode_output_dir): shutil.rmtree(episode_output_dir)
    os.makedirs(audio_temp_dir); os.makedirs(image_temp_dir)
    
    final_video_path = None
    try:
        print("--- BÆ¯á»šC 1: NGHIÃŠN Cá»¨U & VIáº¾T Ká»ŠCH Báº¢N ---")
        content = get_content_from_url(URL_INPUT)
        if not content: raise ValueError("KhÃ´ng láº¥y Ä‘Æ°á»£c ná»™i dung.")
        script_text = generate_podcast_script_from_content(content)
        if not script_text: raise ValueError("KhÃ´ng táº¡o Ä‘Æ°á»£c ká»‹ch báº£n.")
        print("   -> Ká»‹ch báº£n Ä‘Ã£ Ä‘Æ°á»£c táº¡o:\n---\n" + script_text + "\n---")

        print("\n--- BÆ¯á»šC 2: Táº O STORYBOARD ---")
        storyboard_scenes = create_storyboard_from_script(script_text, OVERALL_STYLE)
        if not storyboard_scenes: raise ValueError("KhÃ´ng táº¡o Ä‘Æ°á»£c storyboard há»£p lá»‡.")
        
        print("\n--- BÆ¯á»šC 3: Sáº¢N XUáº¤T Ã‚M THANH & HÃŒNH áº¢NH ---")
        audio_clips_info, image_paths = [], []
        for i, scene in enumerate(storyboard_scenes):
            print(f"\n-> Äang xá»­ lÃ½ Cáº£nh {i+1}/{len(storyboard_scenes)}...")
            try:
                dialogue, host, prompt = scene['dialogue'], scene['host'], scene['image_prompt']
                audio_path = os.path.join(audio_temp_dir, f"line_{i:03d}.wav")
                text_to_speech(dialogue, host, audio_path)
                image_path = os.path.join(image_temp_dir, f"img_{i:03d}.png")
                generated_img = generate_image_locally(prompt, image_path)
                if generated_img and os.path.exists(audio_path):
                    audio_clips_info.append({'path': audio_path, 'duration': AudioSegment.from_wav(audio_path).duration_seconds})
                    image_paths.append(generated_img)
                    print(f"   -> âœ… HoÃ n thÃ nh xá»­ lÃ½ Cáº£nh {i+1}.")
                else: raise ValueError("KhÃ´ng thá»ƒ táº¡o audio hoáº·c image.")
            except Exception as scene_error: print(f"   -> âš ï¸ Bá» qua Cáº£nh {i+1} do lá»—i: {scene_error}")
        
        print(f"\n--- BÆ¯á»šC 4: Dá»°NG VIDEO ({len(image_paths)} cáº£nh há»£p lá»‡) ---")
        if not image_paths: raise ValueError("KhÃ´ng cÃ³ cáº£nh nÃ o há»£p lá»‡ Ä‘á»ƒ dá»±ng video.")
        final_video_path = os.path.join(episode_output_dir, f"{EPISODE_NAME}.mp4")
        assemble_video_with_ffmpeg(audio_clips_info, image_paths, ASSETS_DIR, final_video_path)
        
        print("\n\nğŸ‰ğŸ‰ğŸ‰ HOÃ€N Táº¤T! ğŸ‰ğŸ‰ğŸ‰")
        print(f"Video Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {os.path.abspath(final_video_path)}")

    except Exception as e:
        print(f"\nğŸ›‘ğŸ›‘ğŸ›‘ Lá»–I DÃ‚Y CHUYá»€N: {e}")

# ====================================================
# PHáº¦N 4: ÄIá»‚M Báº®T Äáº¦U CHÆ¯Æ NG TRÃŒNH
# ====================================================
if __name__ == "__main__":
    try:
        load_ai_models()
        run_production_pipeline()
    except Exception as e:
        print(f"\nMá»™t lá»—i nghiÃªm trá»ng Ä‘Ã£ xáº£y ra khi khá»Ÿi Ä‘á»™ng: {e}")